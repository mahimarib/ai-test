{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 500\n"
     ]
    }
   ],
   "source": [
    "num_of_actions = env.action_space.n\n",
    "num_of_states = env.observation_space.n\n",
    "print(num_of_actions, num_of_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "q_table = np.zeros((num_of_states, num_of_actions))\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amount of games we want it to play to test\n",
    "total_episodes = 50000\n",
    "# total amount of test games we want to use\n",
    "total_test_episodes = 100\n",
    "# the max amount of actions it can make in a game\n",
    "max_steps = 99\n",
    "learning_rate = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discount Rate\n",
    "\n",
    "We define a discount rate called gamma. It must be between\n",
    "0 and 1.\n",
    "\n",
    "The larger the gamma, the smaller the discount. This means\n",
    "the learning agent cares more about the long term reward.\n",
    "On the other hand, the smaller the gamma, the bigger the\n",
    "discount. This means our agent cares more about the short\n",
    "term reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.618 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration Rate\n",
    "\n",
    "The higher the exploration rate (epsilon) the more random\n",
    "the actions are in order to explore the environment, this is\n",
    "to find the bigger reward rather than exploiting the known\n",
    "information to maximize the reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.0\n",
    "# the maximum rate, this will be the rate in the beginning\n",
    "max_epsilon = 1.0\n",
    "# this will the minimum rate, which will be at the end\n",
    "min_epsilon = 0.01\n",
    "# exploration decay rate for epsilon, we want more exporation\n",
    "# in the beginning to know the environment, then exploit later\n",
    "# to maximize the reward\n",
    "rate_decay = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(total_episodes):\n",
    "    state = env.reset()\n",
    "    exploration_exploitation = random.uniform(0, 1)\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # if the tradeoff is bigger than the epsilon then it will do\n",
    "        # the action with the greatest valu for the state\n",
    "        if exploration_exploitation > epsilon:\n",
    "            action = np.argmax(q_table[state][:])\n",
    "        # if not it will choose randomly (this will happen for the\n",
    "        # the very first time)\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        # updating the bellman equation\n",
    "        # Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        q_table[state][action] = q_table[state][action] + learning_rate * (reward + gamma * np.max(q_table[new_state][:]) - q_table[state][action])\n",
    "            \n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    # updating the epsilon because we don't need the decay rate to be \n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-rate_decay * episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'qtable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c6e17f6a577c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Take the action (index) that have the maximum expected future reward given that state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qtable' is not defined"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "rewards = []\n",
    "\n",
    "for episode in range(total_test_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    #print(\"****************************************************\")\n",
    "    #print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # UNCOMMENT IT IF YOU WANT TO SEE OUR AGENT PLAYING\n",
    "        env.render()\n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = np.argmax(q_table[state,:])\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "        if done:\n",
    "            rewards.append(total_rewards)\n",
    "            #print (\"Score\", total_rewards)\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_test_episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
